Now that we established the basic principles of how gain and loss get counted, and devised a preference hierarchy, let
us delve into how a situation can be assessed locally by an individual agent. This of course can be considered redundant
given that the mechanism itself is the object of interest here. But having this full-fledged
% TODO: Is that correct to call it "proof of concept"?
model we will be able to run a simulation and see how the mechanism performs, thus get ourselves a proof of concept.

In essence, what is required to be done here is to model a way how an agent assesses the impact of its "hit", "run",
"gather", or "idle" actions in terms of "enemy resource deprivation", "resource acquisition", and other contexts (Figure
\ref{fig:df-models}). Here is the tuple we employ to represent the model formally:

\begin{equation}
    \begin{gathered}
        Model = \left< P, Act, Asp, Tp, E, T, G_{t, a}^{(Int)} \right>,\\
        G: P, Act \times Asp \times Tp \times E \times E \times T \rightarrow \mathbb{R},
    \end{gathered}
\end{equation}

\begin{itemize}
    \item $P$ is a set of model \textit{properties},
    \item $Act = \{Run, Gather, Hit, Idle\}$ is a set of \textit{actions},
    \item $Asp = \{ ERD, RA, EW, SA, SS, RS \}$ is a set of \textit{aspects}; the abbreviations are composed using first letters of strategic aspects (Figure \ref{fig:df-models}),
    \item $E \subseteq \mathbb{R}$ is an \textit{"attribute"} representing energy of an agent,
    \item $Tp = \{Resource, Hitter \}$ is a type of an agent,
    \item $T \subseteq \mathbb{Z}$ is the number of ticks spent by the time the gain function is being calculated counting from the beginning of a current iteration,
    \item $G_{t, a}^{(Int)}$ is the gain function.
\end{itemize}

Here is the list of the elements of the model's properties set $P$. The properties are chosen once and remain stable
during the simulation:

\begin{itemize}
    \item $P_{GnEnWt}$ stands for "gain energy waiting". It represents the amount of energy an agent gains while being $Idle$;
    \item $P_{LsEnMv}$ stands for "loss energy moving". It represents the amount of energy an agent loses when it moves;
    \item $P_{Sp}$ stands for "speed". It represents the speed in \textit{distance units per tick};
    \item $P_{LsEnAg}$ stands for "loss energy aggressive". It represents the fraction of energy the agent loses when it instantiates a hit, i.e. interacts while being in $Hit$ state;
    \item $P_{GnEnWn}$ stands for "gain energy win". It represents the fraction of an enemy's energy if that has been defeated;
    \item $P_{GnRsWn}$ stands for "gain resource win". It represents the fraction of an enemy's energy that is converted into resource, given that the enemy has been defeated;
    \item $P_{LsRsLs}$ stands for "loss resource lose". It represents the amount of resource the team loses when agent gets defeated;
    \item $P_{GnEn}$ stands for "gain energy". It represents the fraction of a $Resource$'s energy that is being converted to energy for the agent;
    \item $P_{GnRs}$ stands for "gain ressource". It represents the fraction of a $Resource$'s energy that is converted to the team's resource.
\end{itemize}

This rather big set of properties reveals the rules of the simulation and provide hints on how the agents' reasoning is
implemented. Now to the score couting. The most straightworward and quite illustrative one is the $G_t^{(Mv)}$ component
of equation (\ref{eq:expected-gain}):

\begin{equation*}
    \begin{gathered}
        $$
        G_t^{(Mv)}(P,act,asp,t) =
        \begin{cases}
            tP_{GnEnWt} & \mbox{if } act=Idle \wedge asp = SA,\\
            ({tP_{LsEnMv}})^{-1} & \mbox{if } act \in \{Hit, Gather, Run\} \wedge asp = SS,\\
            0 & \mbox{otherwise}.
        \end{cases} \\
        act \in Act, \  asp \in Asp.
        $$
    \end{gathered}
\end{equation*}

Simply speaking movement-related scores only make sense in aspects of "strength saving" ($SS$) or "strength acquisition"
($SA$). Also, the approach we have selected does not imply balance of loss and gain, i.e. no negative gains were
implied. This makes for clearer score interpretation in the context of AHP preference graph weights, on one hand, and
for simpler weights normalization, on the other.

Scores for $SS$ and $RS$ aspects are represented as a \textit{loss score} to the power of $-1$. $G_{t,a}^{(Int)}$ values
for "saving" aspects are calculated the same way as it will be demonstrated. So in essence this is reasoning in terms of
loss minimization. This inversion of units here does not create distortions from the standpoint of dimensional analysis
because all the scores for $Idle$, $Hit$, and other actions are "contained" within one aspect and expressed in the same
units.

Interaction score $G_t^{(Int)}$ counting is way more interesting. We employed brobabilistic reasoning here along with
penalties for aggressive behavior, i.e. when an agent is in $Hit$ state, and also adjustments of energy for already
spent simulation ticks during a current iteration.

For sake of clarity, we give one particular team a central role and refer to other teams as rival teams. Agents from the
"focus" team, we call "these agents", and agents from other teams are called "the other" ones.

Ticks spent during a current iteration are taken into account in the form of energy adjustments. $E'$ and $E'_a$
represent "this" and "the other" agents' energy values before the interation. As for interactions between agents from
different teams, the gained scores from the interactions are calculated using probabilities of win of this $p^{(win)}$
and win of the other $p^{(win)}_a$ agent. Also, this agent cannot know what state the other agent will be in by the
moment of interaction. Therefore, every possiblility is considered equally probable. So the interaction outcome is
calculated as an expected value for 4 equally probable states of the other agent.

\begin{equation*}
    \begin{gathered}
        G_t^{(Int)}(P, act, asp, t, E, E_a, Hitter, Hitter) = \begin{cases}
            0.25 \sum_{act_a \in Act} {E'_ap^{(win)}P_{LsRsLs}}  & \mbox{if  } asp = ERD,\\
            0.25 \sum_{act_a \in Act} {E'_ap^{(win)}P_{GnRsWn}}  & \mbox{if  } asp = RA,\\
            0.25 \sum_{act_a \in Act} {E'_ap^{(win)}}  & \mbox{if  } asp = EW,\\
            0.25 \sum_{act_a \in Act} {E'_ap^{(win)}P_{GnEnWn}}  & \mbox{if  } asp = SA,\\
            0.25 \sum_{act_a \in Act} {(E'p_a^{(win)}P_{GnEnWn})^{-1}}  & \mbox{if  } asp = SS,\\
            0.25 \sum_{act_a \in Act} {(E'p_a^{(win)}P_{GnRsWn})^{-1}}  & \mbox{if  } asp = RS,
        \end{cases} \\
        E'(act) = \begin{cases}
            E - tP_{LsEnMv} & \mbox{if } act \in \{Hit, Run, Gather\},\\
            E + tP_{GnEnWt} & \mbox{if } act = $Idle$,
        \end{cases}\\
        E_a'(act_a) = \begin{cases}
            E_a - tP_{LsEnMv} & \mbox{if } act_a \in \{Hit, Run, Gather\},\\
            E_a + tP_{GnEnWt} & \mbox{if } act_a = $Idle$,
        \end{cases}\\
        p^{(win)} = \frac {E'} {E' + E'_a},\ \
        p^{(win)}_a = \frac {E'_a} {E' + E'_a}.
    \end{gathered}
\end{equation*}

And the last type of score is the gain from interactions of a $Hitter$ agent with a resource agent:
\begin{equation*}
    \begin{gathered}
        G_t^{(Int)}(P, act, asp, t, E_a, Hitter, Resource) = \begin{cases}
            {E'_aP_{GnRs}}  & \mbox{if  } asp = RA,\\
            {E'_aP_{GnEn}}  & \mbox{if  } asp = SA.
        \end{cases}
    \end{gathered}
\end{equation*}

You may have noted that gain from fight interactions gets calculated regardless of this agent's actions, whether it
attacks or pursues a resource. It represents the intuitive fact that an agent can be drawn into a fight even against its
"will".

The last thing that should be said about low-level agents' reasoning is how a set of reachable agents $A$ mentioned in
equation (\ref{eq:expected-gain}) is formed. It follows quite simple rules. Other agents should not be from the same
team as this agent. Speed of this agent is derived from its activity ($0$ or $P_Sp$). Speed of the other agent is always
considered $P_Sp$. The number of ticks $N_t$ this agent has in its disposal is adjusted for the energy value of this
agent, and it depends on the parameter $P_LsEnMv$. If this agent is in $Gather$ state, all reachable rival agents are of
interest for this agent. Otherwise, this is the case only for rival agents of type $Hitter$. These rules seem obvious
and straightforward as we all have spatial intuition, therefore, we omit the mathematical form of those for compactness
and simplicity. You may look for formation of the set of reachable (of rather interactable) agents in the source code
for this simulation \cite{github}.
