Now that we established the basic principles of how gain and loss get counted, and devised a preference hierarchy, let
us delve into how a situation can be assessed locally by an individual agent. This of course can be considered redundant
given that the mechanism itself is the object of interest here. But having this full-fledged
% TODO: Is that correct to call it "proof of concept"?
model we will be able to run a simulation and see how the mechanism performs, thus get ourselves a proof of concept.

In essence, what is required to be done here is to model a way how an agent assesses the impact of its "hit", "run",
"gather", or "idle" actions in terms of "enemy resource deprivation", "resource acquisition", and other contexts (Figure
\ref{fig:df-models}). Here is the tuple we employ to represent the model formally:

\begin{equation}
    \begin{gathered}
        Model = \left< P, Act, Asp, Tp, E, T, G_{t, a}^{(Int)} \right>,\\
        G: P, Act \times Asp \times Tp \times E \times E \times T \rightarrow \mathbb{R},
    \end{gathered}
\end{equation}

\begin{itemize}
    \item $P$ is a set of model \textit{properties},
    \item $Act = \{Run, Gather, Hit, Idle\}$ is a set of \textit{actions},
    \item $Asp = \{ ERD, RA, EW, SA, SS, RS \}$ is a set of \textit{aspects}; the abbreviations are composed using first letters of strategic aspects (Figure \ref{fig:df-models}),
    \item $E \subseteq \mathbb{R}$ is an \textit{"attribute"} representing energy of an agent,
    \item $Tp = \{Resource, Hitter \}$ is a type of an agent,
    \item $T \subseteq \mathbb{Z}$ is the number of ticks spent by the time the gain function is being calculated counting from the beginning of a current iteration,
    \item $G_{t, a}^{(Int)}$ is the gain function.
\end{itemize}

Here is the list of the elements of the model's properties set $P$. The properties are chosen once and remain stable
during the simulation:

\begin{itemize}
    \item $P_{GnEnWt}$ stands for "gain energy waiting". It represents the amount of energy an agent gains while being $Idle$;
    \item $P_{LsEnMv}$ stands for "loss energy moving". It represents the amount of energy an agent loses when it moves;
    \item $P_{Sp}$ stands for "speed". It represents the speed in \textit{distance units per tick};
    \item $P_{LsEnAg}$ stands for "loss energy aggressive". It represents the fraction of energy the agent loses when it instantiates a hit, i.e. interacts while being in $Hit$ state;
    \item $P_{GnEnWn}$ stands for "gain energy win". It represents the fraction of an enemy's energy if that has been defeated;
    \item $P_{GnRsWn}$ stands for "gain resource win". It represents the fraction of an enemy's energy that is converted into resource, given that the enemy has been defeated;
    \item $P_{LsRsLs}$ stands for "loss resource lose". It represents the amount of resource the team loses when agent gets defeated;
    \item $P_{GnEn}$ stands for "gain energy". It represents the fraction of a $Resource$'s energy that is being converted to energy for the agent;
    \item $P_{GnRs}$ stands for "gain ressource". It represents the fraction of a $Resource$'s energy that is converted to the team's resource.
\end{itemize}

This rather big set of properties reveals the rules of the simulation and provide hints on how the agents' reasoning is
implemented. Now to the score couting. The most straightworward and quite illustrative one is the $G_t^{(Mv)}$ component
of equation (\ref{eq:expected-gain}):

\begin{equation*}
    \begin{gathered}
        $$
        G_t^{(Mv)}(P,act,asp,t) =
        \begin{cases}
            tP_{GnEnWt} & \mbox{if } act=Idle \wedge asp = SA,\\
            \frac {1} {tP_{LsEnMv}} & \mbox{if } act \in \{Hit, Gather, Run\} \wedge asp = SS,\\
            0 & \mbox{otherwise}.
        \end{cases} \\
        act \in Act, \  asp \in Asp.
        $$
    \end{gathered}
\end{equation*}

Simply speaking movement-related scores only make sense in aspects of "strength saving" ($SS$) or "strength acquisition"
($SA$). Also, the approach we have selected does not imply balance of loss and gain, i.e. no negative gains were
implied. This makes for clearer score interpretation in the context of AHP preference graph weights, on one hand, and
for simpler weights normalization, on the other.

Scores for $SS$ and $RS$ aspects are represented as a \textit{loss score} to the power of $-1$. $G_{t,a}^{(Int)}$ values
for "saving" aspects are calculated the same way as it will be demonstrated. So in essence this is reasoning in terms of
loss minimization. This inversion of units here does not create distortions from the standpoint of dimensional analysis
because all the scores for $Idle$, $Hit$, and other actions are "contained" within one aspect and expressed in the same
units.
